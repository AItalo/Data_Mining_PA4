{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 4\n",
    "\n",
    "This program will build sample classifiers for the pre-processed automobile dataset created for PA1. \n",
    "\n",
    "We will use mean value replacement to resolve missing values, as described by the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Random Instances: Linear Regression\n",
    "\n",
    "This step will create a classifier that predicts mpg values using least squares linear regression.\n",
    "\n",
    "Our predictor attribute will be vehicle weight. The algorithm will take a set of instances, predict their MPG values, and then discretize these values based on the DOE classifications given in PA2.\n",
    "\n",
    "To test our classifier, we will select 5 random instances from the dataset, and then compare our predicted MPG classification with the actual classification from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will first define several helper functions in order to create and clean our dataset.\n",
    "\n",
    "* `read_data()`\n",
    "    * Reads a text file for the data to create a dataset from\n",
    "    * **Parameters**\n",
    "        * `filename`: The name of the file to read data from\n",
    "    * **Returns** \n",
    "        * A string containing the text from the given file\n",
    "* `create_dataset()`\n",
    "    * Turns a formatted string into a 2D dataset array\n",
    "    * **Parameters**\n",
    "        * `data`: A string containing the data to build the dataset from\n",
    "    * **Returns**\n",
    "        * The data in the dataset as a 2D array\n",
    "* `resolve_missing_values()`\n",
    "    * Resolves all instances of \"NA\" by replacing them with the mean of that attribute, in-place\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def create_dataset(data):\n",
    "    data_r = data.splitlines()\n",
    "    dataset = []\n",
    "    for line in data_r:\n",
    "        instance = line.split(',')\n",
    "        dataset.append(instance)\n",
    "    for instance in dataset:\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                instance[i] = float(instance[i])\n",
    "            except:\n",
    "                instance[i] = instance[i]\n",
    "    return dataset\n",
    "\n",
    "def resolve_missing_values(data):\n",
    "    for i in range(10):\n",
    "        if i != 8:\n",
    "            sum_i = 0\n",
    "            count_i = 0\n",
    "            for instance in data:\n",
    "                if instance[i] != \"NA\":\n",
    "                    try:\n",
    "                        sum_i += instance[i]\n",
    "                        count_i += 1\n",
    "                    except:\n",
    "                        print(instance[i])\n",
    "            if count_i == 0:\n",
    "                continue\n",
    "            mean = sum_i / count_i\n",
    "            for instance in data:\n",
    "                if instance[i] == \"NA\":\n",
    "                    instance[i] = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these functions defined, we will call them on the \"auto-data.txt\" file to create the dataset for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(read_data(\"auto-data.txt\"))\n",
    "resolve_missing_values(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a dataset that we can use for the rest of this project.\n",
    "\n",
    "Now, we will define functions for linear regression. To do this, we will copy the helper functions written for PA3, outlined here for clarity.\n",
    "\n",
    "* `compute_total_instances()`  \n",
    "    * **Params**: \n",
    "        * `data` = the dataset to query\n",
    "    * **Returns**: The number of instances in that dataset\n",
    "* `compute_most_common_element()`\n",
    "    * **Params**: \n",
    "        * `data` = the dataset to query\n",
    "        * `index` = the index of the attribute to query\n",
    "    * **Returns**: The most commonly occurring element of that attribute in the dataset\n",
    "* `sum_attribute()`\n",
    "    * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `index` = the index of the attribute to query\n",
    "    * **Returns**:\n",
    "        * The sum of all values for the given index in the dataset\n",
    "* `mean_attribute()`\n",
    "    * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `index` = the index of the attribute to query\n",
    "    * **Returns**:\n",
    "        * The mean of all values of the given attribute in the dataset\n",
    "* `linear_regression_slope()`\n",
    "    * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `x_index` = the index of the attribute to use for the x values\n",
    "        * `y_index` = the index of the attribute to use for the y values\n",
    "    * **Returns**:\n",
    "        * The slope of the linear regression line\n",
    "* `lienar_regression_intercept()`\n",
    "     * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `x_index` = the index of the attribute to use for the x values\n",
    "        * `y_index` = the index of the attribute to use for the y values\n",
    "    * **Returns**:\n",
    "        * The intercept of the linear regression line\n",
    "        \n",
    "Additionally, we will use these formulas for linear regression:\n",
    "* **Linear Regression**:  $\\overline{y} = m\\overline{x} + b$\n",
    "* **Slope**:  $m = \\frac{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})^{2}}$\n",
    "* **Intercept**: $\\overline{y} - m\\overline{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_instances(data):\n",
    "    return len(data)\n",
    "\n",
    "def compute_most_common_element(data, index):\n",
    "    freqs = {}\n",
    "    for instance in data:\n",
    "        if instance[index] in freqs:\n",
    "            freqs[instance[index]] += 1\n",
    "        else:\n",
    "            freqs[instance[index]] = 1            \n",
    "    # line referenced from https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "    mce = max(freqs.items(), key=operator.itemgetter(1))[0]\n",
    "    return mce\n",
    "\n",
    "def sum_attribute(data, index):\n",
    "        sum = 0\n",
    "        for instance in data:\n",
    "            sum += instance[index]\n",
    "        return sum\n",
    "    \n",
    "def mean_attribute(data, index):\n",
    "    mean = sum_attribute(data, index) / compute_total_instances(data)\n",
    "    return mean\n",
    "    \n",
    "    \n",
    "def linear_regression_slope(data, x_index, y_index):\n",
    "    mean_x = mean_attribute(data, x_index)\n",
    "    mean_y = mean_attribute(data, y_index)\n",
    "    sum_dividend = 0\n",
    "    sum_divisor = 0\n",
    "    for i in range(len(data)):\n",
    "        sum_dividend += (data[i][x_index] - mean_x)*(data[i][y_index] - mean_y)\n",
    "        sum_divisor += (data[i][x_index] - mean_x)**2\n",
    "    m = sum_dividend / sum_divisor\n",
    "    return m\n",
    "\n",
    "def linear_regression_intercept(data, x_index, y_index):\n",
    "    mean_x = mean_attribute(data, x_index)\n",
    "    mean_y = mean_attribute(data, y_index)\n",
    "    m = linear_regression_slope(data, x_index, y_index)\n",
    "    b = mean_y - (mean_x * m)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DOE Classifications, we will use these values, defined in PA2:\n",
    "\n",
    "| Rating | MPG   |\n",
    "|--------|-----  |\n",
    "|   10   | ≥ 45  |\n",
    "|   9    | 37-44 |\n",
    "|   8    | 31-36 |\n",
    "|   7    | 27-30 |\n",
    "|   6    | 24-26 |\n",
    "|   5    | 20-23 |\n",
    "|   4    | 17-19 |\n",
    "|   3    | 15-16 |\n",
    "|   2    |   14  |\n",
    "|   1    | ≤ 13  |\n",
    "\n",
    "To make our predictions, we will define our classifier as follows:\n",
    "* `mpg_to_DOE()`\n",
    "    * **Parameters**\n",
    "        * `mpg`: The actual MPG value of a car instance\n",
    "    * **Returns**\n",
    "        * The DOE classification for the given MPG value\n",
    "* `linear_classifier()`\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to use for defining our linear regression\n",
    "        * `test`: A list of test instances to make predictions on\n",
    "    * **Returns**\n",
    "        * A list of the DOE classification for the predicted MPG values of each test instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpg_to_DOE(mpg):\n",
    "    if mpg >= 45:\n",
    "        y = 10\n",
    "    elif mpg >= 37:\n",
    "        y = 9\n",
    "    elif mpg >= 31:\n",
    "        y = 8\n",
    "    elif mpg >= 27:\n",
    "        y = 7\n",
    "    elif mpg >= 24:\n",
    "        y = 6\n",
    "    elif mpg >= 20:\n",
    "        y = 5\n",
    "    elif mpg >= 17:\n",
    "        y = 4\n",
    "    elif mpg >= 15:\n",
    "        y = 3\n",
    "    elif mpg >= 14:\n",
    "        y = 2\n",
    "    else:\n",
    "        y = 1\n",
    "    return y\n",
    "\n",
    "def linear_classifier(data, test):\n",
    "    m = linear_regression_slope(data, 4, 0)\n",
    "    b = linear_regression_intercept(data, 4, 0)\n",
    "    predictions = []\n",
    "    for instance in test:\n",
    "        x = instance[4]\n",
    "        mpg = m*x + b\n",
    "        y = mpg_to_DOE(mpg)\n",
    "        predictions.append(y)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this, we will select a random 5 instances from our dataset to predict on, generated using the following helper function:\n",
    "\n",
    "* `generate_random_instances()`\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to pull random instances from\n",
    "        * `n`: The number of instances to generate\n",
    "    * **Returns**\n",
    "        * A list of _n_ random instances from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import copy\n",
    "\n",
    "def generate_random_instances(data, n):\n",
    "    data_c = copy.deepcopy(data)\n",
    "    test_instances = []\n",
    "    for i in range(n):\n",
    "        index = randint(0, len(data_c)-1)\n",
    "        instance = data_c.pop(index)\n",
    "        test_instances.append(instance)\n",
    "    return test_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run the classifier function on the test instances and display the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = generate_random_instances(dataset, 5)\n",
    "predictions_1 = linear_classifier(dataset, test_instances)\n",
    "\n",
    "def print_output_1(test_instances, predictions):\n",
    "    print(\"===========================================\")\n",
    "    print(\"STEP 1: Linear Regression MPG Classifier\")\n",
    "    print(\"===========================================\")\n",
    "    for i in range(len(predictions)):\n",
    "        print(\"instance: \", end=\"\")\n",
    "        test = copy.deepcopy(test_instances[i])\n",
    "        mpg = test.pop(0)\n",
    "        actual = mpg_to_DOE(mpg)\n",
    "        instance_string = \"\"\n",
    "        for attribute in test:\n",
    "            instance_string += str(attribute)\n",
    "            instance_string += \", \"\n",
    "        instance_string.rstrip(\", \")\n",
    "        print(instance_string)\n",
    "        print(\"class:\", predictions[i], end=\"\")\n",
    "        print(\", actual:\", actual)\n",
    "\n",
    "print_output_1(test_instances, predictions_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Random Instances: kNN\n",
    "\n",
    "For this step, we will instead create a nearest neighbor classifier for mpg instead of linear regression.\n",
    "\n",
    "To do this, we will need several helper functions for computing kNN:\n",
    "\n",
    "* `normalize_instance()`\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to use for calculating the min and max values for normalization\n",
    "        * `instance`: The instance of the dataset to normalize\n",
    "        * `class_index`: The index of the classification attribute, which should not be normalized\n",
    "    * **Returns**\n",
    "        * The given instance, normalized\n",
    "* `calculate_distance()`\n",
    "    * **Parameters**\n",
    "        * `instance`: The instance to calculate the distance to\n",
    "        * `base`: The instance to calculate the distance from\n",
    "    * **Returns**\n",
    "        * The distance between the two instances, using Euclidean Distance Formula\n",
    "* `calculate_all_distances()`\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to calculate distances against\n",
    "        * `base`: The instance to calculate distances for\n",
    "        * `class_index`: The index of the classification attribute, which is not included in the distance formula\n",
    "    * **Returns**\n",
    "        * A list of all distances to the base instance, along with their corresponding indices in the dataset\n",
    "* `knn()`\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to use for knn calculations\n",
    "        * `instance`: The instance to classify\n",
    "        * `k`: The k value for kNN (how many neighbors to use for prediction)\n",
    "        * `class_index`: The index of the classification attribute, which is not included in the distance formula\n",
    "    * **Returns**\n",
    "        * The predicted classification of `instance`, based on kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def normalize_instance(data, instance, class_index):\n",
    "    normalized_instance = []\n",
    "    for i in range(len(instance)):\n",
    "        if i == class_index:\n",
    "            normalized_instance.append(instance[i])\n",
    "            continue\n",
    "        xs = []\n",
    "        for line in data:\n",
    "            xs.append(line[i])\n",
    "        x = instance[i]\n",
    "        normal = (x - min(xs)) / (max(xs) - min(xs))\n",
    "        normalized_instance.append(normal)\n",
    "    return normalized_instance\n",
    "\n",
    "def calculate_distance(instance, base):\n",
    "    sum_a = 0\n",
    "    for i in range(len(instance)):\n",
    "        dif = instance[i] - base[i]\n",
    "        sum_a += dif**2\n",
    "    distance = sum_a**(1/2)\n",
    "    return distance\n",
    "\n",
    "def calculate_all_distances(data, base, class_index):\n",
    "    distances = []\n",
    "    for i in range(len(data)):\n",
    "        instance = copy.deepcopy(data[i])\n",
    "        instance.pop(class_index)\n",
    "        distance = calculate_distance(instance, base)\n",
    "        distances.append((i, distance))\n",
    "    return distances\n",
    "\n",
    "def knn(data, instance, k, class_index):\n",
    "    instance = normalize_instance(data, instance, class_index)\n",
    "    data_n = []\n",
    "    for line in data:\n",
    "        line_n = normalize_instance(data, line, class_index)\n",
    "        data_n.append(line_n)\n",
    "    distances = calculate_all_distances(data_n, instance, class_index)\n",
    "    sorted_distances = sorted(distances, key=operator.itemgetter(1))\n",
    "    neighbor_indices = []\n",
    "    for i in range(k):\n",
    "        neighbor_indices.append(sorted_distances[i][0])\n",
    "    neighbor_classes = []\n",
    "    for i in neighbor_indices:\n",
    "        neighbor_classes.append(data[i][class_index])\n",
    "    class_list = [[x] for x in neighbor_classes]\n",
    "    return compute_most_common_element(class_list, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our classifier in a similar way to Step 1. First, we will create a curtailed dataset that only contains the attributes we will use for distance calculations, which simplifies our logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_step2_data(data):\n",
    "    data_o = []\n",
    "    for instance in data:\n",
    "        instance_o = []\n",
    "        instance_o.append(instance[1])\n",
    "        instance_o.append(instance[4])\n",
    "        instance_o.append(instance[5])\n",
    "        instance_o.append(mpg_to_DOE(instance[0]))\n",
    "        data_o.append(instance_o)\n",
    "    return data_o\n",
    "\n",
    "def knn_classifier(data, test, k, class_index):\n",
    "    predictions = []\n",
    "    for instance in test:\n",
    "        predictions.append(knn(data, instance, k, class_index))\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate an output that runs our classifier on a set of 5 test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "STEP 2: k=5 Nearest Neighbor MPG Classifier\n",
      "===========================================\n",
      "instance: 8.0, 360.0, 215.0, 4615.0, 14.0, 70.0, 1.0, \"ford f250\", 3214.0, \n",
      "class: 1, actual: 1\n",
      "instance: 6.0, 232.0, 100.0, 2914.0, 16.0, 75.0, 1.0, \"amc gremlin\", 2798.0, \n",
      "class: 5, actual: 5\n",
      "instance: 4.0, 90.0, 70.0, 1937.0, 14.2, 76.0, 2.0, \"vw rabbit\", 3499.0, \n",
      "class: 7, actual: 7\n",
      "instance: 4.0, 113.0, 95.0, 2278.0, 15.5, 72.0, 3.0, \"toyota corona hardtop\", 2532.0, \n",
      "class: 6, actual: 6\n",
      "instance: 6.0, 168.0, 120.0, 3820.0, 16.7, 76.0, 2.0, \"mercedes-benz 280s\", 17124.0, \n",
      "class: 3, actual: 3\n"
     ]
    }
   ],
   "source": [
    "data_2 = create_step2_data(dataset)\n",
    "knn_test_instances_raw = generate_random_instances(dataset, 5)\n",
    "knn_test_instances = create_step2_data(knn_test_instances_raw)\n",
    "predictions_2 = knn_classifier(data_2, knn_test_instances, 5, 3)\n",
    "\n",
    "def print_output_2(test_instances, predictions):\n",
    "    print(\"===========================================\")\n",
    "    print(\"STEP 2: k=5 Nearest Neighbor MPG Classifier\")\n",
    "    print(\"===========================================\")\n",
    "    for i in range(len(predictions)):\n",
    "        print(\"instance: \", end=\"\")\n",
    "        test = copy.deepcopy(test_instances[i])\n",
    "        mpg = test.pop(0)\n",
    "        actual = mpg_to_DOE(mpg)\n",
    "        instance_string = \"\"\n",
    "        for attribute in test:\n",
    "            instance_string += str(attribute)\n",
    "            instance_string += \", \"\n",
    "        instance_string.rstrip(\", \")\n",
    "        print(instance_string)\n",
    "        print(\"class:\", predictions[i], end=\"\")\n",
    "        print(\", actual:\", actual)\n",
    "        \n",
    "frprint_output_2(knn_test_instances_raw, predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def compute_accuracy(predictions, actual):\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == actual[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "def compute_error(predictions, actual):\n",
    "    incorrect = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != actual[i]:\n",
    "            incorrect += 1\n",
    "    error = incorrect / len(predictions)\n",
    "    return error\n",
    "\n",
    "def compute_linear(test, train):\n",
    "    predictions = linear_classifier(train, test)\n",
    "    actual = [mpg_to_DOE(instance[0]) for instance in test]\n",
    "    accuracy = compute_accuracy(predictions, actual)\n",
    "    error_rate = compute_error(predictions, actual)\n",
    "    return accuracy, error_rate\n",
    "\n",
    "def compute_knn(test, train, k):\n",
    "    data = create_step2_data(train)\n",
    "    test_instances = create_step2_data(test)\n",
    "    predictions = knn_classifier(data, test_instances, k, 3)\n",
    "    actual = [instance[3] for instance in test_instances]\n",
    "    accuracy = compute_accuracy(predictions, actual)\n",
    "    error_rate = compute_error(predictions, actual)\n",
    "    return accuracy, error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "STEP 3: Predictive Accuracy\n",
      "===========================================\n",
      "Random Subsample (k=10 2:1 Train/Test)\n",
      "Linear Regression: accuracy = 0.42, error rate = 0.58\n",
      "k Nearest Neighbor: accuracy = 0.37, error rate = 0.63\n"
     ]
    }
   ],
   "source": [
    "def create_random_subsample(data, size):\n",
    "    cutoff = int(len(data) * size)\n",
    "    data_c = copy.deepcopy(data)\n",
    "    shuffle(data_c)\n",
    "    train = data_c[:cutoff]\n",
    "    test = data_c[cutoff + 1:]\n",
    "    return test, train\n",
    "    \n",
    "def print_output_3(data, k, train_size, test_size):\n",
    "    size = (test_size) / (test_size + train_size)\n",
    "    sum_accuracy_l = 0\n",
    "    sum_accuracy_k = 0\n",
    "    sum_error_l = 0\n",
    "    sum_error_k = 0\n",
    "    for i in range(k):\n",
    "        test, train = create_random_subsample(data, size)\n",
    "        accuracy_l, error_l = compute_linear(test, train)\n",
    "        accuracy_k, error_k = compute_knn(test, train, 5)\n",
    "        sum_accuracy_l += accuracy_l\n",
    "        sum_error_l += error_l\n",
    "        sum_accuracy_k += accuracy_k\n",
    "        sum_error_k += error_k\n",
    "    mean_accuracy_l = sum_accuracy_l / k\n",
    "    mean_accuracy_k = sum_accuracy_k / k\n",
    "    mean_error_l = sum_error_l / k\n",
    "    mean_error_k = sum_error_k / k\n",
    "    print(\"===========================================\")\n",
    "    print(\"STEP 3: Predictive Accuracy\")\n",
    "    print(\"===========================================\")\n",
    "    print(\"Random Subsample (k=\", k, \" \", train_size, \":\", test_size, \" Train/Test)\", sep=\"\")\n",
    "    print(\"Linear Regression: accuracy = %.2f, error rate = %.2f\" %(mean_accuracy_l, mean_error_l))\n",
    "    print(\"k Nearest Neighbor: accuracy = %.2f, error rate = %.2f\" %(mean_accuracy_k, mean_error_k))\n",
    "    \n",
    "    \n",
    "print_output_3(dataset, 10, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "STEP 4: Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 10-Fold Cross Validation\n",
      "Linear Regression: accuracy = 0.41, error rate = 0.59\n",
      "k Nearest Neighbor: accuracy = 0.34, error rate = 0.66\n"
     ]
    }
   ],
   "source": [
    "def create_cross_fold(data, n):\n",
    "    data_r = copy.deepcopy(data)\n",
    "    shuffle(data_r)\n",
    "    size = int(len(data) * 1/n) \n",
    "    start = 0\n",
    "    end = size\n",
    "    folds = []\n",
    "    for i in range(n-1):\n",
    "        folds.append(data[start:end])\n",
    "        start = end + 1\n",
    "        end += size + 1\n",
    "    folds.append(data[start:])\n",
    "    return folds\n",
    "\n",
    "def print_output_4(data, n):\n",
    "    sum_accuracy_l = 0\n",
    "    sum_accuracy_k = 0\n",
    "    sum_error_l = 0\n",
    "    sum_error_k = 0\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        accuracy_l, error_l = compute_linear(test, train)\n",
    "        accuracy_k, error_k = compute_knn(test, train, 5)\n",
    "        sum_accuracy_l += accuracy_l\n",
    "        sum_error_l += error_l\n",
    "        sum_accuracy_k += accuracy_k\n",
    "        sum_error_k += error_k\n",
    "    mean_accuracy_l = sum_accuracy_l / n\n",
    "    mean_accuracy_k = sum_accuracy_k / n\n",
    "    mean_error_l = sum_error_l / n\n",
    "    mean_error_k = sum_error_k / n\n",
    "    print(\"===========================================\")\n",
    "    print(\"STEP 4: Predictive Accuracy\")\n",
    "    print(\"===========================================\")\n",
    "    print(\"Stratified \", n, \"-Fold Cross Validation\", sep=\"\")\n",
    "    print(\"Linear Regression: accuracy = %.2f, error rate = %.2f\" %(mean_accuracy_l, mean_error_l))\n",
    "    print(\"k Nearest Neighbor: accuracy = %.2f, error rate = %.2f\" %(mean_accuracy_k, mean_error_k))\n",
    "    \n",
    "print_output_4(dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (Stratefied10-Fold Cross Validation Results):\n",
      "===============================================================\n",
      "  MPG    1    2    3    4    5    6    7    8    9    10    Total    Recognition (%)\n",
      "-----  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  -----------------\n",
      "    1   22    1    6    2    1    0    0    0    0     0       32            68.75\n",
      "    2    9    5    3    2    1    0    0    0    0     0       20            25\n",
      "    3   10    5    8   13    2    0    0    0    0     0       38            21.0526\n",
      "    4    0    1    8   21   21    3    0    0    0     0       54            38.8889\n",
      "    5    0    2    3   10   28   19    1    0    0     0       63            44.4444\n",
      "    6    0    0    0    1    7   25    8    0    0     0       41            60.9756\n",
      "    7    0    0    0    0    3   12   18    0    0     0       33            54.5455\n",
      "    8    0    0    0    0    0    2   20    0    0     0       22             0\n",
      "    9    0    0    0    0    0    1    2    0    0     0        3             0\n",
      "   10    0    0    0    0    0    0    0    0    0     0        0             0\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def confusion_matrix_linear(data, n):\n",
    "    results = []\n",
    "    for i in range(10):\n",
    "        row = [i + 1]\n",
    "        for i in range(10):\n",
    "            row.append(0)\n",
    "        results.append(row)\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        predictions = linear_classifier(train, test)\n",
    "        actual = [mpg_to_DOE(instance[0]) for instance in test]\n",
    "        for i in range(len(actual)):\n",
    "            results[actual[i]-1][predictions[i]] += 1\n",
    "    for row in results:\n",
    "        row.append(sum(row) - row[0])\n",
    "        try:\n",
    "            recognition = (row[row[0]] / row[11])*100\n",
    "        except ZeroDivisionError:\n",
    "            recognition = 0\n",
    "        finally:\n",
    "            row.append(recognition)\n",
    "    header = [\"MPG\"]\n",
    "    for i in range(1, 11):\n",
    "        header.append(str(i))\n",
    "    header += [\"Total\", \"Recognition (%)\"]\n",
    "    print(\"Linear Regression (Stratefied\", n, \"-Fold Cross Validation Results):\", sep=\"\")\n",
    "    print(\"===============================================================\")\n",
    "    print(tabulate(results, headers=header))\n",
    "    \n",
    "confusion_matrix_linear(dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN (Stratefied10-Fold Cross Validation Results):\n",
      "========================================================\n",
      "  MPG    1    2    3    4    5    6    7    8    9    10    Total    Recognition (%)\n",
      "-----  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  -----------------\n",
      "    1   14    6    6    4    2    0    0    0    0     0       32            43.75\n",
      "    2    4    1   14    1    0    0    0    0    0     0       20             5\n",
      "    3   10    6    6   11    5    0    0    0    0     0       38            15.7895\n",
      "    4    8    2    7   19   16    2    0    0    0     0       54            35.1852\n",
      "    5    4    0    2   20   20   15    1    1    0     0       63            31.746\n",
      "    6    0    0    0    2    9   21    7    2    0     0       41            51.2195\n",
      "    7    0    0    0    1    1   11   11    9    0     0       33            33.3333\n",
      "    8    0    0    0    0    0    2    8   12    0     0       22            54.5455\n",
      "    9    0    0    0    0    0    1    1    1    0     0        3             0\n",
      "   10    0    0    0    0    0    0    0    0    0     0        0             0\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix_knn(data, n, k):\n",
    "    results = []\n",
    "    for i in range(10):\n",
    "        row = [i + 1]\n",
    "        for i in range(10):\n",
    "            row.append(0)\n",
    "        results.append(row)\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        data = create_step2_data(train)\n",
    "        test_instances = create_step2_data(test)\n",
    "        predictions = knn_classifier(data, test_instances, k, 3)\n",
    "        actual = [instance[3] for instance in test_instances]\n",
    "        for i in range(len(actual)):\n",
    "            results[actual[i]-1][predictions[i]] += 1\n",
    "    for row in results:\n",
    "        row.append(sum(row) - row[0])\n",
    "        try:\n",
    "            recognition = (row[row[0]] / row[11])*100\n",
    "        except ZeroDivisionError:\n",
    "            recognition = 0\n",
    "        finally:\n",
    "            row.append(recognition)\n",
    "    header = [\"MPG\"]\n",
    "    for i in range(1, 11):\n",
    "        header.append(str(i))\n",
    "    header += [\"Total\", \"Recognition (%)\"]\n",
    "    print(\"k-NN (Stratefied\", n, \"-Fold Cross Validation Results):\", sep=\"\")\n",
    "    print(\"========================================================\")\n",
    "    print(tabulate(results, headers=header))\n",
    "    \n",
    "confusion_matrix_knn(dataset, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
