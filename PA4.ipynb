{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 4\n",
    "\n",
    "This program will build sample classifiers for the pre-processed automobile dataset created for PA1. \n",
    "\n",
    "We will use mean value replacement to resolve missing values, as described by the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Random Instances: Linear Regression\n",
    "\n",
    "This step will create a classifier that predicts mpg values using least squares linear regression.\n",
    "\n",
    "Our predictor attribute will be vehicle weight. The algorithm will take a set of instances, predict their MPG values, and then discretize these values based on the DOE classifications given in PA2.\n",
    "\n",
    "To test our classifier, we will select 5 random instances from the dataset, and then compare our predicted MPG classification with the actual classification from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will first define several helper functions in order to create and clean our dataset.\n",
    "\n",
    "* `read_data()`\n",
    "    * Reads a text file for the data to create a dataset from\n",
    "    * **Parameters**\n",
    "        * `filename`: The name of the file to read data from\n",
    "    * **Returns** \n",
    "        * A string containing the text from the given file\n",
    "* `create_dataset()`\n",
    "    * Turns a formatted string into a 2D dataset array\n",
    "    * **Parameters**\n",
    "        * `data`: A string containing the data to build the dataset from\n",
    "    * **Returns**\n",
    "        * The data in the dataset as a 2D array\n",
    "* `resolve_missing_values()`\n",
    "    * Resolves all instances of \"NA\" by replacing them with the mean of that attribute, in-place\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def create_dataset(data):\n",
    "    data_r = data.splitlines()\n",
    "    dataset = []\n",
    "    for line in data_r:\n",
    "        instance = line.split(',')\n",
    "        dataset.append(instance)\n",
    "    for instance in dataset:\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                instance[i] = float(instance[i])\n",
    "            except:\n",
    "                instance[i] = instance[i]\n",
    "    return dataset\n",
    "\n",
    "def resolve_missing_values(data):\n",
    "    for i in range(10):\n",
    "        if i != 8:\n",
    "            sum_i = 0\n",
    "            count_i = 0\n",
    "            for instance in data:\n",
    "                if instance[i] != \"NA\":\n",
    "                    try:\n",
    "                        sum_i += instance[i]\n",
    "                        count_i += 1\n",
    "                    except:\n",
    "                        print(instance[i])\n",
    "            if count_i == 0:\n",
    "                continue\n",
    "            mean = sum_i / count_i\n",
    "            for instance in data:\n",
    "                if instance[i] == \"NA\":\n",
    "                    instance[i] = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these functions defined, we will call them on the \"auto-data.txt\" file to create the dataset for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(read_data(\"auto-data.txt\"))\n",
    "resolve_missing_values(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a dataset that we can use for the rest of this project.\n",
    "\n",
    "Now, we will define functions for linear regression. To do this, we will copy the helper functions written for PA3, outlined here for clarity.\n",
    "\n",
    "* `compute_total_instances()`  \n",
    "    * **Params**: \n",
    "        * `data` = the dataset to query\n",
    "    * **Returns**: The number of instances in that dataset\n",
    "* `sum_attribute()`\n",
    "    * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `index` = the index of the attribute to query\n",
    "    * **Returns**:\n",
    "        * The sum of all values for the given index in the dataset\n",
    "* `mean_attribute()`\n",
    "    * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `index` = the index of the attribute to query\n",
    "    * **Returns**:\n",
    "        * The mean of all values of the given attribute in the dataset\n",
    "* `linear_regression_slope()`\n",
    "    * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `x_index` = the index of the attribute to use for the x values\n",
    "        * `y_index` = the index of the attribute to use for the y values\n",
    "    * **Returns**:\n",
    "        * The slope of the linear regression line\n",
    "* `lienar_regression_intercept()`\n",
    "     * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `x_index` = the index of the attribute to use for the x values\n",
    "        * `y_index` = the index of the attribute to use for the y values\n",
    "    * **Returns**:\n",
    "        * The intercept of the linear regression line\n",
    "* `linear_regression_correlation()`\n",
    "     * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `x_index` = the index of the attribute to use for the x values\n",
    "        * `y_index` = the index of the attribute to use for the y values\n",
    "    * **Returns**:\n",
    "        * The correlation coefficient of the linear regression line\n",
    "* `linear_regression_std_error()`\n",
    "    * **Params**:\n",
    "        * `data` = the dataset to query\n",
    "        * `y_index` = the index of the attribute to use for the y values\n",
    "    * **Returns**:\n",
    "        * The Standard Error of the linear regression line\n",
    "        \n",
    "        \n",
    "Additionally, we will use these formulas for linear regression:\n",
    "* **Linear Regression**:  $\\overline{y} = m\\overline{x} + b$\n",
    "* **Slope**:  $m = \\frac{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})^{2}}$\n",
    "* **Intercept**: $\\overline{y} - m\\overline{x}$\n",
    "* **Correlation Coefficient**: $r = \\frac{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sqrt{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})^{2} \\sum_{i = 1}^{n}(y_{i}-\\overline{y})^{2}}}$\n",
    "* **Standard Error**: $\\sqrt{\\frac{\\sum_{i = 1}^{n}(y_{i} - \\overline{y}^{2}}{n}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_instances(data):\n",
    "    return len(data)\n",
    "\n",
    "def sum_attribute(data, index):\n",
    "        sum = 0\n",
    "        for instance in data:\n",
    "            sum += instance[index]\n",
    "        return sum\n",
    "    \n",
    "def mean_attribute(data, index):\n",
    "    mean = sum_attribute(data, index) / compute_total_instances(data)\n",
    "    return mean\n",
    "    \n",
    "    \n",
    "def linear_regression_slope(data, x_index, y_index):\n",
    "    mean_x = mean_attribute(data, x_index)\n",
    "    mean_y = mean_attribute(data, y_index)\n",
    "    sum_dividend = 0\n",
    "    sum_divisor = 0\n",
    "    for i in range(len(data)):\n",
    "        sum_dividend += (data[i][x_index] - mean_x)*(data[i][y_index] - mean_y)\n",
    "        sum_divisor += (data[i][x_index] - mean_x)**2\n",
    "    m = sum_dividend / sum_divisor\n",
    "    return m\n",
    "\n",
    "def linear_regression_intercept(data, x_index, y_index):\n",
    "    mean_x = mean_attribute(data, x_index)\n",
    "    mean_y = mean_attribute(data, y_index)\n",
    "    m = linear_regression_slope(data, x_index, y_index)\n",
    "    b = mean_y - (mean_x * m)\n",
    "    return b\n",
    "\n",
    "def linear_regression_correlation(data, x_index, y_index):\n",
    "    mean_x = mean_attribute(data, x_index)\n",
    "    mean_y = mean_attribute(data, y_index)\n",
    "    sum_dividend = 0\n",
    "    sum_divisor_x = 0\n",
    "    sum_divisor_y = 0\n",
    "    for i in range(len(data)):\n",
    "        sum_dividend += (data[i][x_index] - mean_x)*(data[i][y_index] - mean_y)\n",
    "        sum_divisor_x += ((data[i][x_index] - mean_x)**2)\n",
    "        sum_divisor_y += ((data[i][y_index] - mean_y)**2)\n",
    "    sum_divisor = sum_divisor_x * sum_divisor_y\n",
    "    divisor = np.sqrt(sum_divisor)\n",
    "    r = sum_dividend / divisor\n",
    "    return r\n",
    "\n",
    "def linear_regression_std_error(data, y_index):\n",
    "        mean_y = mean_attribute(data, y_index)\n",
    "        sum_dividend = 0\n",
    "        for i in range(len(data)):\n",
    "            sum_dividend += (data[i][y_index] - mean_y) ** 2\n",
    "        std_sqr = sum_dividend / compute_total_instances(data)\n",
    "        std_error = np.sqrt(std_sqr)\n",
    "        return std_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DOE Classifications, we will use these values, defined in PA2:\n",
    "\n",
    "| Rating | MPG   |\n",
    "|--------|-----  |\n",
    "|   10   | â‰¥ 45  |\n",
    "|   9    | 37-44 |\n",
    "|   8    | 31-36 |\n",
    "|   7    | 27-30 |\n",
    "|   6    | 24-26 |\n",
    "|   5    | 20-23 |\n",
    "|   4    | 17-19 |\n",
    "|   3    | 15-16 |\n",
    "|   2    |   14  |\n",
    "|   1    | â‰¤ 13  |\n",
    "\n",
    "To make our predictions, we will define our classifier as follows:\n",
    "* `classifier()`\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to use for defining our linear regression\n",
    "        * `test`: A list of test instances to make predictions on\n",
    "    * **Returns**\n",
    "        * A list of the DOE classification for the predicted MPG values of each test instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(data, test):\n",
    "    m = linear_regression_slope(data, 4, 0)\n",
    "    b = linear_regression_intercept(data, 4, 0)\n",
    "    predictions = []\n",
    "    for instance in test:\n",
    "        x = instance[4]\n",
    "        mpg = m*x + b\n",
    "        if mpg >= 45:\n",
    "            y = 10\n",
    "        elif mpg >= 37:\n",
    "            y = 9\n",
    "        elif mpg >= 31:\n",
    "            y = 8\n",
    "        elif mpg >= 27:\n",
    "            y = 7\n",
    "        elif mpg >= 24:\n",
    "            y = 6\n",
    "        elif mpg >= 20:\n",
    "            y = 5\n",
    "        elif mpg >= 17:\n",
    "            y = 4\n",
    "        elif mpg >= 15:\n",
    "            y = 3\n",
    "        elif mpg >= 14:\n",
    "            y = 2\n",
    "        else:\n",
    "            y = 1\n",
    "        predictions.append(y)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this, we will select a random 5 instances from our dataset to predict on, generated using the following helper function:\n",
    "\n",
    "* `generate_random_instances()`\n",
    "    * **Parameters**\n",
    "        * `data`: The dataset to pull random instances from\n",
    "        * `n`: The number of instances to generate\n",
    "    * **Returns**\n",
    "        * A list of _n_ random instances from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def generate_random_instances(data, n):\n",
    "    test_indices = []\n",
    "    while len(test_indices) < n:\n",
    "        index = randint(0, len(data) - 1)\n",
    "        if index not in test_indices:\n",
    "            test_indices.append(index)\n",
    "    instances = []\n",
    "    for i in test_indices:\n",
    "        instances.append(data[i])\n",
    "    return instances\n",
    "\n",
    "test_instances = generate_random_instances(dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run the classifier function on the test instances and display the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "STEP 1: Linear Regression MPG Classifier\n",
      "===========================================\n",
      "instance: 8.0, 350.0, 165.0, 4142.0, 11.5, 70.0, 1.0, \"chevrolet chevelle concours (sw)\", 3210.0, \n",
      "class: 2, actual: 5\n",
      "instance: 6.0, 173.0, 115.0, 2595.0, 11.3, 79.0, 1.0, \"chevrolet citation\", 4112.573033707865, \n",
      "class: 6, actual: 7\n",
      "instance: 4.0, 134.0, 95.0, 2560.0, 14.2, 78.0, 3.0, \"toyota corona\", 4574.0, \n",
      "class: 6, actual: 7\n",
      "instance: 8.0, 307.0, 130.0, 3504.0, 12.0, 70.0, 1.0, \"chevrolet chevelle malibu\", 2881.0, \n",
      "class: 4, actual: 4\n",
      "instance: 6.0, 225.0, 110.0, 3620.0, 18.7, 78.0, 1.0, \"dodge aspen\", 3911.0, \n",
      "class: 4, actual: 4\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier(dataset, test_instances)\n",
    "\n",
    "def print_output_1(test_instances, predictions):\n",
    "    print(\"===========================================\")\n",
    "    print(\"STEP 1: Linear Regression MPG Classifier\")\n",
    "    print(\"===========================================\")\n",
    "    for i in range(len(predictions)):\n",
    "        print(\"instance: \", end=\"\")\n",
    "        mpg = test_instances[i].pop(0)\n",
    "        if mpg >= 45:\n",
    "            actual = 10\n",
    "        elif mpg >= 37:\n",
    "            actual = 9\n",
    "        elif mpg >= 31:\n",
    "            actual = 8\n",
    "        elif mpg >= 27:\n",
    "            actual = 7\n",
    "        elif mpg >= 24:\n",
    "            actual = 6\n",
    "        elif mpg >= 20:\n",
    "            actual = 5\n",
    "        elif mpg >= 17:\n",
    "            actual = 4\n",
    "        elif mpg >= 15:\n",
    "            actual = 3\n",
    "        elif mpg >= 14:\n",
    "            actual = 2\n",
    "        else:\n",
    "            actual = 1\n",
    "        instance_string = \"\"\n",
    "        for attribute in test_instances[i]:\n",
    "            instance_string += str(attribute)\n",
    "            instance_string += \", \"\n",
    "        instance_string.rstrip(\", \")\n",
    "        print(instance_string)\n",
    "        print(\"class:\", predictions[i], end=\"\")\n",
    "        print(\", actual:\", actual)\n",
    "\n",
    "print_output_1(test_instances, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Random Instances: kNN\n",
    "\n",
    "For this step, we will instead create a nearest neighbor classifier for mpg instead of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
